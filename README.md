# Computer-Vision-Project
Action recognition

1. Project Idea:
To recognize the action in a video such as Bending, Jumping Jack, One handed wave, Two handed Wave or Jumping in place with the help of a classifier using the similitude moments of the Motion History Images (MHIs) and Motion Energy Images (MEIs). 

2. Introduction and Methodology:
Human action recognition is a challenging area of research in the field of Computer Vision. Although, Neural Network and Deep Learning techniques exist to efficiently recognize and classify different human actions, the focus of this project is to achieve the same using conventional Computer Vision techniques and see if we get comparable results.
a. Frame Extraction:
The dataset consists of 3-4 second videos of different people performing different actions. To be able to perform subsequent tasks, the frames are extracted from the video using the OpenCV VideoCapture method. On an average, 80 frames were extracted from the videos. For example, below are the extracted frames for Daria performing the Jumping Jacks activity.

![image](https://user-images.githubusercontent.com/42225976/155924481-3e947c92-becf-499b-bf47-c47740b4acca.png)

b. Background Subtraction I
The corresponding backgrounds for the different individuals are available in the dataset. Since different videos have different backgrounds, we use the backgrounds based on the person performing the activity. The images of the backgrounds from the video files are also obtained by frame extraction. After identifying the correct background, the silhouettes are generated using the Background Subtraction I technique.

![image](https://user-images.githubusercontent.com/42225976/155924558-05a9c295-594c-4bc5-85f0-cf9b798a1243.png)

Various thresholds were used for Background Subtraction I. The Threshold of 40 resulted in a considerable amount of noise reduction and only minimal loss of data and hence it is chosen as the best threshold to generate Background Subtracted Binary Images. The background subtraction of the same person (Daria) performing the same activity (Jumping Jack) as mentioned in the Frame Extraction is as shown,

![image](https://user-images.githubusercontent.com/42225976/155924617-d6d13dee-2bf8-4ae2-b57f-9e5c48fc04aa.png)

c. Motion History Image and Motion Energy Image
The Motion History Image (MHI) records information about the latest frame number at which motion was observed at a pixel. With this representation we would be able to uniquely identify and represent the different activities being performed. The Motion Energy Image (MEI) is a binary representation which records information about the presence of motion at each pixel. The temporal template is a combination of MHI and the MEI i.e. TT = [MHI, MEI]. The MHI works on the principle that whenever there is a motion assign a particular timestamp to all the non-zero pixels in the frame and discard all the other frames which are less than the minimum frames to be considered.

![image](https://user-images.githubusercontent.com/42225976/155924696-9eeb3d13-3a6c-419b-9510-fb1f30e68e2f.png)

This creates a fading intensity of frames and represents the progression of “how” the motion is happening. The MHI needs to be (re)normalized to 0-1 for matching which can be done using the formula,

![image](https://user-images.githubusercontent.com/42225976/155924763-e793454d-8738-4f93-a3c0-b6718a053867.png)

Since, the MEI is just the cumulative motion images, it can be obtained by thresholding the MHI,

![image](https://user-images.githubusercontent.com/42225976/155924809-a6dca014-a4f2-426f-81a7-cde4f6660a04.png)

Motion History Images for a few actions are as shown

![image](https://user-images.githubusercontent.com/42225976/155924860-311e5fc8-1032-4531-8e0f-bea00f01bb10.png)

d. Similitude Moments
A Feature vector of 14 Similitude moments, 7 each for MEIs and MHIs is calculated using the following formula,

![image](https://user-images.githubusercontent.com/42225976/155924919-f8bcc258-558a-4ebd-b4e4-bf6d262ff83a.png)

The advantages of using similitude moments to generate the feature vector are
● Translational Invariance: The people performing the activities might be standing at different positions in the video.
● Scale Invariance: The size of the people performing the activities may vary.

e. Classification
Given two feature vectors generated by the calculation of the similitude moments on the MHI and MEI, we would be able to find the similarity between them by computing the distance between them. The smaller the distance between the vectors, the higher the similarity between them. For this purpose we could use the following distances.
● Euclidean distance: One metric used for classification is the Euclidean distance metric which is nothing but the simple linear distance between the two vectors calculated using the L2 norm.

![image](https://user-images.githubusercontent.com/42225976/155925002-8e665747-907d-4bcb-ad8d-d8a9abf9d7f0.png)

● Cosine Similarity: Another metric for classification is the Cosine similarity, which is just the measure of the cosine of the angle of the two vectors. The cosine distance is
calculated as 1 - cosine similarity.

![image](https://user-images.githubusercontent.com/42225976/155925099-2aab3ce0-6674-4038-983a-c71f97afdb8e.png)

3. Outcomes

![image](https://user-images.githubusercontent.com/42225976/155925193-f66f4e56-eca2-411c-aab2-63a871fc6aeb.png)

Apart from the existing five actions, the  methodology was also tried on classifying actions where the person moves across the scene such as walking, running and skipping. The results obtained were not great (Accuracy - 44.4 %) as the MHIs obtained for these activities are extremely similar, resulting in very close distance values and hence, a lot of misclassifications.

![image](https://user-images.githubusercontent.com/42225976/155925291-c3aed674-b7e0-4e02-ba2e-e9b16096b188.png)
